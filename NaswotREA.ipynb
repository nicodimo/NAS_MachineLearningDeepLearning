{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichelePresti/NAS_MachineLearningDeepLearning/blob/main/NaswotREA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Nq2-kqUpgm"
      },
      "source": [
        "#Define Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ofIogUx_T3lD"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "max_uid = 15625\n",
        "#@title ##Configuration Info { run: \"auto\" }\n",
        "#configuration by param\n",
        "dataset = \"ImageNet16\" #@param {type:\"string\"} [\"cifar10\", \"cifar100\", \"ImageNet16\"]\n",
        "run_id =  1# @param {type:\"integer\"}\n",
        "trial =  2#@param {type:\"integer\"}\n",
        "n_random =  10#@param {type:\"integer\"}\n",
        "point = '2a' # @param ['2a', '2b']\n",
        "imagenet_path = 'Use only if dataset=Imagenet16' #@param{type:\"string\"}\n",
        "use_default_path = True #@param{type:\"boolean\"}\n",
        "n_evolution = 2#@param{type: \"integer\"}\n",
        "n_arch_distance = 2#@param{type: \"integer\"}\n",
        "n_survivor = 1#@param{type:\"integer\"}\n",
        "population_size = 10#@param{type:\"integer\"}\n",
        "proxy_type = \"ReLU\" #@param {type: \"string\"} [\"ReLU\", \"SynFlow\"]\n",
        "\n",
        "\n",
        "config['score'] = 'hook_logdet'\n",
        "config['nasspace'] = 'nasbench201'\n",
        "config['augtype'] = 'none'\n",
        "config['dataset'] = dataset\n",
        "config['maxofn'] = 3\n",
        "config['batch_size'] = 128\n",
        "config['seed'] = 1\n",
        "config['run_id'] = run_id\n",
        "config['dataset_id'] = 'CIFAR10'\n",
        "config['start_uid'] = 0 \n",
        "config['stop_uid'] =  15000 \n",
        "config['trial'] = trial\n",
        "config['n_random'] = n_random\n",
        "config['point'] = point\n",
        "config['imagenet_path'] = '/content/drive/MyDrive/ImageNet16' if use_default_path else imagenet_path\n",
        "config['n_evolution'] = n_evolution\n",
        "config['n_arch_distance'] = n_arch_distance\n",
        "config['n_survivor'] = n_survivor\n",
        "config['population_size'] = population_size\n",
        "config['proxy_type'] = proxy_type\n",
        "\n",
        "#max 15625 stop_uid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "h1xetWeZeEDF",
        "outputId": "41bb45d6-9133-4d53-d147-1adaeed3817c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hov87k0BUwXP"
      },
      "source": [
        "#Import NAS Bench API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EVF4reTyT-_g"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/MichelePresti/NAS_MachineLearningDeepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OOSiDI1bUBNA"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/NAS_MachineLearningDeepLearning/neural_model ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W2IG2PhLUCaw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_arch_config_by_dataset(dataset) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function return the architectures config by dataset in a pandas dataframe.\n",
        "    PARAMETERS:\n",
        "       dataset= string among [cifar10, cifar100, imaginet]\n",
        "    \"\"\"\n",
        "    if(dataset == 'cifar10'):\n",
        "        df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__CIFAR10_config.csv', header=0)\n",
        "        return df\n",
        "    if(dataset == 'cifar100'):\n",
        "      df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__CIFAR100_config.csv', header=0)\n",
        "      return df\n",
        "    if(dataset == 'ImageNet16'):\n",
        "      df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/nas_bench_201__ImageNet16_config.csv', header=0)\n",
        "      return df\n",
        "    else: \n",
        "      print('Dataset name not valid')\n",
        "      return None\n",
        "\n",
        "def get_standard_config(csv_config: pd.DataFrame) -> dict:\n",
        "    res = {}\n",
        "    res['name'] = csv_config.iloc[0]['name']\n",
        "    res['C'] = csv_config.iloc[0]['C']\n",
        "    res['N'] = csv_config.iloc[0]['N']\n",
        "    res['arch_str'] = csv_config.iloc[0]['arch_str']\n",
        "    res['num_classes'] = 1\n",
        "    return res\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1oGWDNmgUCYP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "searchspace = get_arch_config_by_dataset(config['dataset'])\n",
        "searchspace\n",
        "searchspace.to_csv('searchspace_daprocessare.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW5ewvBLU1OE"
      },
      "source": [
        "#Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s65hVshwUCVo"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2019 #\n",
        "##################################################\n",
        "import os, sys, hashlib, torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "\n",
        "def calculate_md5(fpath, chunk_size=1024 * 1024):\n",
        "    md5 = hashlib.md5()\n",
        "    with open(fpath, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            md5.update(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "\n",
        "def check_md5(fpath, md5, **kwargs):\n",
        "    return md5 == calculate_md5(fpath, **kwargs)\n",
        "\n",
        "\n",
        "def check_integrity(fpath, md5=None):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    if md5 is None:\n",
        "        return True\n",
        "    else:\n",
        "        return check_md5(fpath, md5)\n",
        "\n",
        "\n",
        "class ImageNet16(data.Dataset):\n",
        "    # http://image-net.org/download-images\n",
        "    # A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets\n",
        "    # https://arxiv.org/pdf/1707.08819.pdf\n",
        "\n",
        "    train_list = [\n",
        "        [\"train_data_batch_1\", \"27846dcaa50de8e21a7d1a35f30f0e91\"],\n",
        "        [\"train_data_batch_2\", \"c7254a054e0e795c69120a5727050e3f\"],\n",
        "        [\"train_data_batch_3\", \"4333d3df2e5ffb114b05d2ffc19b1e87\"],\n",
        "        [\"train_data_batch_4\", \"1620cdf193304f4a92677b695d70d10f\"],\n",
        "        [\"train_data_batch_5\", \"348b3c2fdbb3940c4e9e834affd3b18d\"],\n",
        "        [\"train_data_batch_6\", \"6e765307c242a1b3d7d5ef9139b48945\"],\n",
        "        [\"train_data_batch_7\", \"564926d8cbf8fc4818ba23d2faac7564\"],\n",
        "        [\"train_data_batch_8\", \"f4755871f718ccb653440b9dd0ebac66\"],\n",
        "        [\"train_data_batch_9\", \"bb6dd660c38c58552125b1a92f86b5d4\"],\n",
        "        [\"train_data_batch_10\", \"8f03f34ac4b42271a294f91bf480f29b\"],\n",
        "    ]\n",
        "    valid_list = [\n",
        "        [\"val_data\", \"3410e3017fdaefba8d5073aaa65e4bd6\"],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, train, transform, use_num_of_class_only=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.train = train  # training set or valid set\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError(\"Dataset not found or corrupted.\")\n",
        "\n",
        "        if self.train:\n",
        "            downloaded_list = self.train_list\n",
        "        else:\n",
        "            downloaded_list = self.valid_list\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        for i, (file_name, checksum) in enumerate(downloaded_list):\n",
        "            file_path = os.path.join(self.root, file_name)\n",
        "            # print ('Load {:}/{:02d}-th : {:}'.format(i, len(downloaded_list), file_path))\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(f)\n",
        "                else:\n",
        "                    entry = pickle.load(f, encoding=\"latin1\")\n",
        "                self.data.append(entry[\"data\"])\n",
        "                self.targets.extend(entry[\"labels\"])\n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 16, 16)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "        if use_num_of_class_only is not None:\n",
        "            assert (\n",
        "                isinstance(use_num_of_class_only, int)\n",
        "                and use_num_of_class_only > 0\n",
        "                and use_num_of_class_only < 1000\n",
        "            ), \"invalid use_num_of_class_only : {:}\".format(use_num_of_class_only)\n",
        "            new_data, new_targets = [], []\n",
        "            for I, L in zip(self.data, self.targets):\n",
        "                if 1 <= L <= use_num_of_class_only:\n",
        "                    new_data.append(I)\n",
        "                    new_targets.append(L)\n",
        "            self.data = new_data\n",
        "            self.targets = new_targets\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{name}({num} images, {classes} classes)\".format(\n",
        "            name=self.__class__.__name__,\n",
        "            num=len(self.data),\n",
        "            classes=len(set(self.targets)),\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index] - 1\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in self.train_list + self.valid_list:\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wpvhhftTUCTS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "def get_dataset(dataset) -> DataLoader:\n",
        "    \"\"\"\n",
        "    This function return the dataset given its name in torch DataLoader format.\n",
        "    PARAMETERS:\n",
        "       dataset= string among [cifar10, cifar100, imaginet]\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset == 'cifar10':\n",
        "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    elif dataset == 'cifar100':\n",
        "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
        "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
        "              transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    elif dataset.startswith('ImageNet16'):\n",
        "        mean = [x / 255 for x in [122.68, 116.66, 104.01]]\n",
        "        std = [x / 255 for x in [63.22, 61.26, 65.09]]\n",
        "        lists = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(16, padding=2), transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean, std)]\n",
        "        transform = transforms.Compose(lists)\n",
        "        trainset = ImageNet16(config['imagenet_path'], True, transform, 120)\n",
        "        train_dt = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'],\n",
        "                                                  shuffle=True, num_workers=2)\n",
        "    else:\n",
        "        raise TypeError(\"Unknow dataset : {:}\".format(dataset))\n",
        "\n",
        "    return train_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xZRyJjrpUCQn"
      },
      "outputs": [],
      "source": [
        "train_dt = get_dataset(config['dataset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7EkocqlU5P7"
      },
      "source": [
        "#Define Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JqoS_7YTUCOC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_batch_jacobian(net, x, target, device, args=None):\n",
        "    net.zero_grad()\n",
        "    x.requires_grad_(True)\n",
        "    y, out = net(x)\n",
        "    y.backward(torch.ones_like(y))\n",
        "    jacob = x.grad.detach()\n",
        "    return jacob, target.detach(), y.detach(), out.detach()\n",
        "\n",
        "\n",
        "def hooklogdet(K, labels=None):\n",
        "    s, ld = np.linalg.slogdet(K)\n",
        "    return ld\n",
        "\n",
        "\n",
        "def score_network(network, x, x2, target, device):\n",
        "    jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "    network(x2.to(device))\n",
        "    value = hooklogdet(network.K, target)\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLWrtL3dU7uf"
      },
      "source": [
        "#Search Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.telegram import tqdm, trange\n",
        "\n",
        "def naswot_search_n2(run_id, dataset_id, device, n):\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    n = config['n_random']\n",
        "    trial = config['trial']\n",
        "    point = config['point']\n",
        "    l = random.sample(range(max_uid), n)\n",
        "    # for uid in tqdm(l,\\\n",
        "    #    token='5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw',\\\n",
        "    #    chat_id='142397010',\\\n",
        "    #    desc=f'result_run{run_id}_dataset{dataset_id}_point_{point}'):\n",
        "    for uid in l:\n",
        "      #i = uid-start_uid\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "          #print('NUM MODULES IN ARCHITECTURES', len(j))\n",
        "          # Starting score algorithm\n",
        "          network = network.to(device)\n",
        "          ## start time\n",
        "          #start = time.time()\n",
        "          random.seed(config['seed'])\n",
        "          np.random.seed(config['seed'])\n",
        "          torch.manual_seed(config['seed'])\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          #print(f'Score (uid {uid}): {np.mean(s)}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "          #print(f'Elapsed time (uid {uid}): {stop-start}')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    df.to_csv(f'./result_run{run_id}_{trial}_{dataset_id}.csv')\n",
        "    \n",
        "    return "
      ],
      "metadata": {
        "id": "pFNEIq332qv_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2a or 2b"
      ],
      "metadata": {
        "id": "gYVmXBTPmyCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/NAS_MachineLearningDeepLearning/Cifar10Result.csv')\n",
        "try: \n",
        "  df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "except:\n",
        "  print(\"Already dropped\")\n",
        "\n",
        "acc_df = df\n"
      ],
      "metadata": {
        "id": "9EfGHBULm08R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.telegram import tqdm, trange\n",
        "\n",
        "\n",
        "def naswot_search_n(run_id, dataset, device, n, trial):\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    n = config['n_random']\n",
        "    #trial = config['trial']\n",
        "    #random.seed(i)\n",
        "    l = random.sample(range(max_uid), n)\n",
        "    best_score, best_acc, best_net = 0, 0, 0\n",
        "    best = {}\n",
        "    best['score'] = 0\n",
        "    best['acc'] = 0\n",
        "    best['uid'] = 0\n",
        "    print(l)\n",
        "    # for uid in tqdm(l,\\\n",
        "    #    token='5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw',\\\n",
        "    #    chat_id='142397010',\\\n",
        "    #    desc=f'result_run{run_id}_{point}_dataset{dataset_id}_'):\n",
        "    for uid in l:\n",
        "      #i = uid-start_uid\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "          #print('NUM MODULES IN ARCHITECTURES', len(j))\n",
        "          # Starting score algorithm\n",
        "          \n",
        "          network = network.to(device)\n",
        "          ## start time\n",
        "          #start = time.time()\n",
        "          #random.seed(config['seed'])\n",
        "          #np.random.seed(config['seed'])\n",
        "          #torch.manual_seed(config['seed'])\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "          print(f'Score (uid {uid}): {np.mean(s)}, Accuracy: {acc}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          score = np.mean(s)\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "\n",
        "          if config['point'] == '2a':\n",
        "          #point 2a\n",
        "            if score > best_score:\n",
        "              best_score = score\n",
        "              best_net = uid\n",
        "              best_acc = acc\n",
        "          else:\n",
        "            #point 2b\n",
        "            \n",
        "            if acc > best_acc:\n",
        "              best_acc = acc\n",
        "              best_score = score\n",
        "              best_net = uid\n",
        "\n",
        "          #print(f'Elapsed time (uid {uid}): {stop-start}')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    df.to_csv(f'./result_run{run_id}_trial{trial}_{point}_{dataset}.csv')\n",
        "    print(f'Best Network: {best_net}; Score: {best_score}; Accuracy: {best_acc}')\n",
        "    best = {}\n",
        "    best['acc'] = best_acc\n",
        "    best['score'] = best_score\n",
        "    best['uid'] = best_net\n",
        "    series_net = pd.Series(best)\n",
        "    series_net.to_csv(f'best_network_{run_id}_trial{trial}_{dataset}.csv')\n",
        "    return "
      ],
      "metadata": {
        "id": "4w8L81_mnF_H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#All Search"
      ],
      "metadata": {
        "id": "fyQKB2AfZmgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "coGyuvqXUCLf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "from neural_model.neural_model import get_cell_net\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.telegram import tqdm, trange\n",
        "\n",
        "def search_all(run_id, dataset_id, device, start_uid, stop_uid):\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    #l = random.sample(range(max_uid), 100)\n",
        "    #for uid in tqdm(range(start_uid, stop_uid)):\n",
        "    for uid in tqdm(range(start_uid, stop_uid),\\\n",
        "       token='5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw',\\\n",
        "       chat_id='142397010',\\\n",
        "       desc=f'result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_'):\n",
        "      i = uid-start_uid\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "\n",
        "          #print('NUM MODULES IN ARCHITECTURES', len(j))\n",
        "          # Starting score algorithm\n",
        "          network = network.to(device)\n",
        "          ## start time\n",
        "          #start = time.time()\n",
        "          random.seed(config['seed'])\n",
        "          np.random.seed(config['seed'])\n",
        "          torch.manual_seed(config['seed'])\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          #print(f'Score (uid {uid}): {np.mean(s)}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "          #print(f'Elapsed time (uid {uid}): {stop-start}')\n",
        "          if i + 1 % 1000 == 0:\n",
        "            df = pd.DataFrame.from_dict(result)\n",
        "            result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "            df.to_csv(f'./NASWOT_result_run{run_id}_dataset{dataset_id}_{i}_.csv')\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    df.to_csv(f'./result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_LastRecords_.csv')\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdWImgk0VCKr"
      },
      "source": [
        "#Save Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bJd_rt9sUCGb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "8ba5e99a-9749-434b-8f55-18b616aa4d68"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a83b26db0d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#path = f'./result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_LastRecords_.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'./result_run{run_id}_{trial}_{dataset_id}.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpath_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'best_network_{run_id}_trial{trial}_{dataset_id}.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_id' is not defined"
          ]
        }
      ],
      "source": [
        "#path = f'./result_run{run_id}_{start_uid//1000}_dataset{dataset_id}_LastRecords_.csv'\n",
        "path = f'./result_run{run_id}_{trial}_{dataset_id}.csv'\n",
        "path_net = f'best_network_{run_id}_trial{trial}_{dataset_id}.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cX8kz0hUCDm"
      },
      "outputs": [],
      "source": [
        "!cp -r $path /content/drive/MyDrive/project\n",
        "!cp -r $path_net /content/drive/MyDrive/project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-3jXt0ghbqj"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# import requests\n",
        "\n",
        "# TOKEN = '5374697833:AAFXE6wumxuSFFwpSzSmEQ6WL4jl8OBVOqw'\n",
        "# CHAT_ID = '142397010'\n",
        "# SEND_URL = f'https://api.telegram.org/bot{TOKEN}/sendMessage'\n",
        "# your_message = f\"RUN COMPLETED!\\nInformation:\\n\\t\\t\\t\\t\\t\\t\\t\\tRun_Id:\\t{run_id}\\n\\t\\t\\t\\t\\t\\t\\t\\tTrial:{trial}\\n\\t\\t\\t\\t\\t\\t\\t\\tDataset_Id:\\t{dataset_id}\"\n",
        "# requests.post(SEND_URL, json={'chat_id': CHAT_ID, 'text': your_message}) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SynFlow Proxy Evolution Algorithm âš› ðŸ‘¾\n",
        "\n"
      ],
      "metadata": {
        "id": "MBMBzGm2oY7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/NAS_MachineLearningDeepLearning/ZeroCostNas ."
      ],
      "metadata": {
        "id": "7bZM7j8xk96w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from neural_model.neural_model import get_cell_net\n",
        "\n",
        "\"\"\"\n",
        "Synflow proxy implementation\n",
        "TO BE IMPLEMENTED\n",
        "\"\"\"\n",
        "from ZeroCostNas.foresight.models import *\n",
        "from ZeroCostNas.foresight.pruners import *\n",
        "from ZeroCostNas.foresight.dataset import *\n",
        "from ZeroCostNas.foresight.weight_initializers import init_net\n",
        "\n",
        "\n",
        "def synflow_search(dataset, device, population) -> pd.DataFrame:\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': [], 'accuracy': []}\n",
        "    for uid in population:\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "          network = network.to(device)\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "          print(f'Score (uid {uid}): {np.mean(s)}, Accuracy: {acc}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['accuracy'].append(acc)\n",
        "          score = np.mean(s)\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    return df"
      ],
      "metadata": {
        "id": "PHlcmbjxoY7S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized Evolution Algorithm NASWOT ðŸ¥‡\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "45pD-uhihnVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from neural_model.neural_model import get_cell_net\n",
        "\n",
        "\"\"\"\n",
        "NAS WOT Algorithm\n",
        "\"\"\"\n",
        "\n",
        "def naswot_search(dataset, device, population) -> pd.DataFrame:\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': [], 'accuracy': []}\n",
        "    for uid in population:\n",
        "      net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "      net_config: dict = get_standard_config(net_config)\n",
        "      network = get_cell_net(net_config)\n",
        "      try:\n",
        "          start = time.time()\n",
        "          if 'hook_' in config['score']:\n",
        "\n",
        "              def counting_forward_hook(module_hook, inp, out):\n",
        "                  try:\n",
        "                      if hasattr(module_hook, 'visited_backwards') and not module_hook.visited_backwards:\n",
        "                          return\n",
        "                      if isinstance(inp, tuple):\n",
        "                          inp = inp[0]\n",
        "                      inp = inp.view(inp.size(0), -1)\n",
        "                      x = (inp > 0).float()\n",
        "                      K = x @ x.t()\n",
        "                      K2 = (1. - x) @ (1. - x.t())\n",
        "                      if hasattr(network, 'K'):\n",
        "                        network.K = network.K + K.cpu().numpy() + K2.cpu().numpy()\n",
        "                      else: \n",
        "                        network.K = K.cpu().numpy() + K2.cpu().numpy()\n",
        "                  except Exception as exception:\n",
        "                      print(exception)\n",
        "                      pass\n",
        "\n",
        "\n",
        "              def counting_backward_hook(module_hook, inp, out):\n",
        "                  module_hook.visited_backwards = True\n",
        "\n",
        "              j = []\n",
        "              for name, module in network.named_modules():\n",
        "                  j.append(name)\n",
        "                  if 'ReLU' in str(type(module)):\n",
        "                      module.register_forward_hook(counting_forward_hook)\n",
        "                      module.register_backward_hook(counting_backward_hook)\n",
        "          network = network.to(device)\n",
        "          s = []\n",
        "          for j in range(config['maxofn']):\n",
        "              data_iterator = iter(train_dt)\n",
        "              x, target = next(data_iterator)\n",
        "              x2 = torch.clone(x)\n",
        "              x2 = x2.to(device)\n",
        "              x, target = x.to(device), target.to(device)\n",
        "              jacobs, labels, y, out = get_batch_jacobian(network, x, target, device, config)\n",
        "              if 'hook_' in config['score']:\n",
        "                  network(x2.to(device))\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "              else:\n",
        "                  value = hooklogdet(network.K, target)\n",
        "                  s.append(value)\n",
        "          acc = acc_df['valid-accuracy'].iloc[uid]\n",
        "          print(f'Score (uid {uid}): {np.mean(s)}, Accuracy: {acc}')\n",
        "          stop = time.time()\n",
        "          result['uid'].append(uid)\n",
        "          result['score'].append(np.mean(s))\n",
        "          result['accuracy'].append(acc)\n",
        "          score = np.mean(s)\n",
        "          result['elapsed_time'].append(stop-start)\n",
        "      except Exception as e:\n",
        "          print(e)\n",
        "    df = pd.DataFrame.from_dict(result)\n",
        "\n",
        "    result = {'uid': [], 'score': [], 'elapsed_time': []}\n",
        "    return df"
      ],
      "metadata": {
        "id": "G18K3jznCcdL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiuTUsbqU_LA"
      },
      "source": [
        "# Random Search Algorithm ðŸŽ³\n",
        "\n",
        "---\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "1. Sample N Random Architectures\n",
        "2. Run Scoring Algorithm On Them\n",
        "3. Choose and store the best performing\n",
        "4. Repeat N Times and then take the best one of the BestArchitecturesArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIh6uy9bUCI4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.environ['WANDB_CONSOLE'] = 'off'\n",
        "start = time.time()\n",
        "\n",
        "run_id = config['run_id']\n",
        "dataset = config['dataset']\n",
        "start_uid = config['start_uid']\n",
        "stop_uid = config['stop_uid']\n",
        "n = config['n_random']    # N size of random sample\n",
        "trial = config['trial']\n",
        "proxy_type = config['proxy_type']\n",
        "survivors = {}\n",
        "\n",
        "print('*******************************')\n",
        "print('Running Random Search algorithm')\n",
        "print('Parameters:')\n",
        "print(f'Dataset: {dataset}')\n",
        "print(f'Num Round: {trial}')\n",
        "print(f'Scoring Algorithm: {proxy_type}')\n",
        "print('*******************************')\n",
        "\n",
        "\n",
        "for i in range(trial):\n",
        "  print(f\"Round {i}\")\n",
        "  population = random.sample(range(max_uid), config['n_random'])\n",
        "  if config['proxy_type'] == 'ReLU':\n",
        "    trained_population = naswot_search(dataset, device, population)\n",
        "  else: \n",
        "    trained_population = synflow_search(dataset, device, population)\n",
        "  trained_population.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "  if len(survivors) > 0:\n",
        "    survivors: pd.DataFrame = survivors.append(trained_population.head(config['n_survivor']), ignore_index=True)\n",
        "  else:\n",
        "    survivors = trained_population.head(config['n_survivor'])\n",
        "\n",
        "survivors.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "survivors = survivors.head(1)\n",
        "\n",
        "survivors.to_csv('RandomSearch.csv')\n",
        "stop = time.time()\n",
        "\n",
        "total_time = stop - start\n",
        "print('*****************************************************************')\n",
        "print(f'Best performing net with RandomSearch')\n",
        "print(tabulate(survivors, headers='keys', tablefmt='psql', showindex=False))\n",
        "print(f'Total time for search over all searchspace: {total_time}')\n",
        "print('*****************************************************************')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Evolution Algorithm âœ¨\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Algorithm Steps\n",
        "\n",
        "\n",
        "1.   Get N Random Architectures Called Population\n",
        "2.   Run Scoring Algorithm On Population\n",
        "3.   Take N Survivor, Choose As The Best Score\n",
        "4.   Create New Generation With Architecture At N Distance From The Survivor\n",
        "5.   Repeat From 2 For N Evolution Era"
      ],
      "metadata": {
        "id": "ETge1xVOpwhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Architecture Configuration Class\n",
        "\"\"\"\n",
        "\n",
        "class Architecture:\n",
        "    def __init__(self, arch):\n",
        "        self.architecture = arch\n",
        "        self.graph = {'0': [], '1':[], '2': []}\n",
        "        for token in arch.split('+'):\n",
        "            tmp = list(filter(lambda x: x != '', token.split('|')))\n",
        "            for x in tmp:\n",
        "                op = x.split('~')\n",
        "                self.graph[str(op[1])].append(op[0])\n",
        "            \n",
        "    def distance(self, obj):\n",
        "        differences = 0\n",
        "        if isinstance(obj, Architecture):\n",
        "            for key in self.graph.keys():\n",
        "                differences += len([i for i, j in zip(self.graph[key], obj.graph[key]) if i != j])\n",
        "            # print('DIFF:' + str(differences))\n",
        "            return differences\n",
        "        else:\n",
        "            print('Invalid Object')\n",
        "            return -1\n",
        "            \n",
        "    def print_architecture(self):\n",
        "        print(self.architecture)"
      ],
      "metadata": {
        "id": "z2G3JAktoY7R"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Find all architectures with a distance n (max), the distance is calculated as Hamming Distance\n",
        "\"\"\"\n",
        "\n",
        "def find_arch_n_dist(survivors, max_dist, anchestors):\n",
        "  population = []\n",
        "  for uid in survivors['uid']:\n",
        "    net_config: pd.DataFrame = searchspace.loc[searchspace['uid'] == uid]\n",
        "    net_config: dict = get_standard_config(net_config)\n",
        "    net_config = Architecture(net_config['arch_str'])\n",
        "    for id in range(0, max_uid):\n",
        "      candidate: pd.DataFrame = searchspace.loc[searchspace['uid'] == id]\n",
        "      candidate: dict = get_standard_config(candidate)\n",
        "      candidate = Architecture(candidate['arch_str'])\n",
        "      dist = net_config.distance(candidate)\n",
        "      # print('Distance: ' + str(dist))\n",
        "      # if id not in anchestors and dist <= max_dist:\n",
        "      if dist <= max_dist:\n",
        "        anchestors.append(id)\n",
        "        population.append(id)\n",
        "\n",
        "  population = random.sample(population, config['population_size']) if len(population) > config['population_size'] else []\n",
        "  population.append(*survivors['uid'])\n",
        "  return population\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "56IMvMlroY7S"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get N Random Samples\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = config['dataset']\n",
        "population = random.sample(range(max_uid), config['n_random'])\n",
        "trial = config['n_evolution']\n",
        "survivors = {}\n",
        "anchestors = population.copy()\n",
        "# Run Algorithm on Population\n",
        "start = time.time()\n",
        "\n",
        "print('*******************************')\n",
        "print('Running Regularized Evolution algorithm')\n",
        "print('Parameters:')\n",
        "print(f'Dataset: {dataset}')\n",
        "print(f'Num Round: {trial}')\n",
        "print(f'Scoring Algorithm: {proxy_type}')\n",
        "print('*******************************')\n",
        "for i in range(config['n_evolution']):\n",
        "  if config['proxy_type'] == 'ReLU':\n",
        "    trained_population = naswot_search(dataset, device, population)\n",
        "  else:\n",
        "    trained_population = synflow_search(dataset, device, population)\n",
        "  print('TRAINED', trained_population)\n",
        "  # Take N Survivor\n",
        "  trained_population.sort_values(by=['score'], ascending=False, inplace=True)\n",
        "  survivors = trained_population.head(config['n_survivor'])\n",
        "  print('SURVIVORS', survivors)\n",
        "  # Create New Generation\n",
        "  if i < trial:\n",
        "    population = find_arch_n_dist(survivors=survivors, max_dist=config['n_arch_distance'], anchestors=anchestors)\n",
        "  else: \n",
        "    break\n",
        "  if len(population) == 0:\n",
        "    break\n",
        "\n",
        "  print(len(population), population)\n",
        "\n",
        "stop = time.time()\n",
        "\n",
        "total_time = stop - start\n",
        "print('*****************************************************************')\n",
        "print(f'Best performing net with RandomSearch')\n",
        "print(tabulate(survivors, headers='keys', tablefmt='psql', showindex=False))\n",
        "print(f'Total time for search over all searchspace: {total_time}')\n",
        "print('*****************************************************************')\n",
        "survivors.to_csv('NASWOT_REA.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L5VGzlhvoY7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NaswotREA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hov87k0BUwXP",
        "CW5ewvBLU1OE",
        "C7EkocqlU5P7",
        "kLWrtL3dU7uf",
        "fyQKB2AfZmgc",
        "SdWImgk0VCKr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}